---
title: 'Lab Report 3'
author: "By: Abhishek Shah, Ravi Seth, Tanya Ralliaram"
geometry: margin=.75in
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
    theme: cosmo
header-includes:
- \usepackage{graphicx}
- \usepackage{color}
graphics: yes
fontsize: 11pt
always_allow_html: true
---


```{r, message=FALSE}
# Insert necessary packages
library('glmnet')
library('boot')
library('caret')
library('ISLR')
library('plotly')
library('gridExtra')
library('tree')
library('rpart')
library('rpart.plot')
library('rattle')
library('MLmetrics')
library('e1071')
```

## Question 1: Classification
```{r}
# Read in data
croissant <- read.csv("data/croissant.csv")[,-1]
circles <- read.csv("data/circles.csv")[,-1]
varied <- read.csv("data/varied.csv")[,-1]
```


### Question 1.1: Preprocess and Plot
```{r}
croissant$y <- as.factor(croissant$y)
circles$y <- as.factor(circles$y)
varied$y <- as.factor(varied$y)

cro <- ggplot(data = croissant) +
  geom_boxplot(aes(x = x1, y=x2, colour=y)) +
  ggtitle("Boxplot of Croissant Data")
cir <- ggplot(data = circles) +
  geom_boxplot(aes(x = x1, y=x2, colour=y)) +
  ggtitle("Boxplot of Circles Data")
var <- ggplot(data = varied) +
  geom_boxplot(aes(x = x1, y=x2, colour=y)) +
  ggtitle("Boxplot of Varied Data")

grid.arrange(cro, cir, var, ncol=2)
```

\pagebreak
### Questions 1.2-1.4 for Croissant Data
```{r}
## Question 1.2
set.seed(112)

train_inds <- sample(1:nrow(croissant), floor(nrow(croissant)*0.5))
train <- croissant[ train_inds, ]
test  <- croissant[-train_inds, ]

y.train <- train$y
x.train <- model.matrix(y ~ .,train)[,-1]
x.test  <- model.matrix(y ~ .,test)[,-1]

## Question 1.3

# Logistic Regression
lreg <- glm(y ~ ., data=train, family = "binomial")
pred1 <- predict(lreg, newdata=as.data.frame(x.test), type = "response") > 0.5
lreg_acc <- mean(pred1 == (test$y==1))
lreg_con <- table(predict=pred1,actual=(test$y))

# Decision Tree
dtree <- rpart(y~., data=train)
pred2 <- predict(dtree, as.data.frame(x.test), type = "class")
dtree_acc <- Accuracy(pred2,test$y)
dtree_con <- table(predict=pred2,actual=(test$y))

# SVM
svmfit <- svm(y~.,data=train, kernel ="radial", gamma=1,cost=1)
pred3 <- predict(svmfit,as.data.frame(x.test), type="class")
print('We chose radial as the kernel as it best fits the shape of the data 
            and thus should lead to a better prediction.')
svm_acc <- Accuracy(pred3,test$y)
svm_con <- table(predict=pred3,actual=(test$y))

g1 <- ggplot(test, aes(x1,x2,colour=y)) +
  geom_point() +
  ggtitle("True Classes")
g2 <- ggplot(test, aes(x1,x2,colour=pred1)) +
  geom_point() +
  ggtitle("Logreg Preds")
g3 <- ggplot(test, aes(x1,x2,colour=pred2)) +
  geom_point() +
  ggtitle("Decision Tree Preds")
g4 <- ggplot(test, aes(x1,x2,colour=pred3)) +
  geom_point() +
  ggtitle("SVM Preds")

grid.arrange(g1,g2,g3,g4,ncol=2)
print('Looking at the four plots, we can see that Logistic Regression has 
      the most misclassifications and that Decision Tree performs pretty well, 
      but not as well as SVM which seems to perform the best.')

sprintf("Logistic Regression Accuracy: %f", lreg_acc) 
sprintf("Decision Tree Accuracy: %f", dtree_acc)
sprintf("SVM Accuracy: %f", svm_acc)
print("In terms of accuracy, SVM has the highest and Decision Tree was second highest. 
      Logistic Regression has the lowest out of the three.")

lreg_con
dtree_con
svm_con
print("SVM was the least biased out of the three as it had zero False Positive (FP) and 
      one False Negatives (FN). Decision Tree has 4 FP and 2 FN and Logistics Regression
      has 12 FP and FN.")

```
```{r}
## Question 1.4 for Croissant Data

# Logistic Regression
print('Logistic Regression')
set.seed(112)
lreg.control <- trainControl(method = 'cv', number = 10)
lreg.cv <- train(y ~ .,
               data = train,
               trControl = lreg.control,
               method = "glm",
               family=binomial())

summary(lreg.cv)
lreg.best <- lreg.cv$finalModel
lreg.best

pred4 <- predict(lreg.cv, test, type = "raw")
lreg_acc <- Accuracy(pred4,test$y)
lreg_con <- table(predict=pred4,actual=(test$y))

# Decision Tree
print('Decision Tree')
set.seed(112)

# perform 10-fold cross validation repeated 3 times
dtree.control = trainControl(method = 'repeatedcv', number = 10, repeats = 3)
dtree.cv <- train(y ~ ., 
                  data = train,
                  method = "rpart",
                  trControl = dtree.control,
                  tuneLength = 15)

summary(dtree.cv)
dtree.best <- dtree.cv$finalModel
dtree.best

pred5 <- predict(dtree.cv, test, type = "raw")
dtree_acc <- Accuracy(pred5,test$y)
dtree_con <- table(predict=pred5,actual=(test$y))

# SVM
set.seed(112)
svmfit <- svm(y~.,data=train, kernel ="radial", gamma=1,cost=1)
tune.out <- tune(svm, y~., data=train, kernel ="radial",
              ranges =list(cost=c(0.01, 0.05, .1 ,1 ,10 ,100 ,1000),
                           gamma=c(0.5,1,2,3,4)))
pred6 <- predict(tune.out$best.model,test)

print('We chose radial as the kernel as it best fits the shape of the data 
            and thus should lead to a better prediction.')
svm_acc <- Accuracy(pred6,test$y)
svm_con <- table(predict=pred6,actual=(test$y))

summary(tune.out)


g1 <- ggplot(test, aes(x1,x2,colour=y)) +
  geom_point() +
  ggtitle("True Classes")
g2 <- ggplot(test, aes(x1,x2,colour=pred4)) +
  geom_point() +
  ggtitle("CV Logistic Regression")
g3 <- ggplot(test, aes(x1,x2,colour=pred5)) +
  geom_point() +
  ggtitle("CV Decision Tree Preds")
g4 <- ggplot(test, aes(x1,x2,colour=pred6)) +
  geom_point() + 
  ggtitle("SVM, CV'd, cost=1, gamma=0.5")

grid.arrange(g1,g2,g3,g4,ncol=2)

sprintf("Logistic Regression Accuracy: %f", lreg_acc) 
sprintf("Decision Tree Accuracy: %f", dtree_acc)
sprintf("SVM Accuracy: %f", svm_acc)


lreg_con
dtree_con
svm_con


```
When Cross Validation was added, SVM still has the highest accuracy, and
Logistic regression still has the lowest. The accuracy for all is very similar to the previous result

For bias, SVM was the least biased out of the three as it had zero False Positive (FP) and 2 False Negatives (FN). Decision Tree has 4 FP and 2 FN and Logistics Regression has 12 FP and 12 FN.

Overall, the accuracy results appear the same as they were without CV, except for SVM which performed slightly worse.

\pagebreak
### Questions 1.2-1.4 for Circle Data
```{r}
## Question 1.2
set.seed(112)

train_inds <- sample(1:nrow(circles), floor(nrow(circles)*0.5))
train <- circles[ train_inds, ]
test  <- circles[-train_inds, ]

y.train <- train$y
x.train <- model.matrix(y ~ .,train)[,-1]
x.test  <- model.matrix(y ~ .,test)[,-1]

## Question 1.3

# Logistic Regression
lreg <- glm(y ~ ., data=train, family = "binomial")
pred1 <- predict(lreg, newdata=as.data.frame(x.test), type = "response") > 0.5
lreg_acc <- mean(pred1 == (test$y==1))
lreg_con <- table(predict=pred1,actual=(test$y))

# Decision Tree
dtree <- rpart(y~., data=train)
pred2 <- predict(dtree, as.data.frame(x.test), type = "class")
dtree_acc <- Accuracy(pred2,test$y)
dtree_con <- table(predict=pred2,actual=(test$y))

# SVM
svmfit <- svm(y~.,data=train, kernel ="radial", gamma=1,cost=1)
pred3 <- predict(svmfit,as.data.frame(x.test), type="class")
print('We chose radial as the kernel as it best fits the shape of the data 
            and thus should lead to a better prediction.')
svm_acc <- Accuracy(pred3,test$y)
svm_con <- table(predict=pred3,actual=(test$y))

g1 <- ggplot(test, aes(x1,x2,colour=y)) +
  geom_point() +
  ggtitle("True Classes")
g2 <- ggplot(test, aes(x1,x2,colour=pred1)) +
  geom_point() +
  ggtitle("Logreg Preds")
g3 <- ggplot(test, aes(x1,x2,colour=pred2)) +
  geom_point() +
  ggtitle("Decision Tree Preds")
g4 <- ggplot(test, aes(x1,x2,colour=pred3)) +
  geom_point() +
  ggtitle("SVM Preds")

grid.arrange(g1,g2,g3,g4,ncol=2)
print('Looking at the four plots, we can see that Logisitc Regression has 
      a lot misclassifications and that Decision Tree performs well, but has noticeable 
      misflassications. Again SVM seems to perform the best.')

sprintf("Logisitc Regression Accuracy: %f", lreg_acc) 
sprintf("Decision Tree Accuracy: %f", dtree_acc)
sprintf("SVM Accuracy: %f", svm_acc)
print("In terms of accuracy, SVM has the highest and Decision Tree was second highest. 
      Logistic Regression has the lowest out of the three.")

lreg_con
dtree_con
svm_con
print("SVM was the least biased out of the three as it had 7 False Positives (FP) and 
      7 False Negatives (FN). Decision Tree has 27 FP and 18 FN and Logistic Regression
      has 202 FP and 45 FN.")
```
```{r}
## Question 1.4 for Circles Data

# Logistic Regression
print('Logistic Regression')
set.seed(112)
lreg.control <- trainControl(method = 'cv', number = 10)
lreg.cv <- train(y ~ .,
               data = train,
               trControl = lreg.control,
               method = "glm",
               family=binomial())

summary(lreg.cv)
lreg.best <- lreg.cv$finalModel
lreg.best

pred4 <- predict(lreg.cv, test, type = "raw")
lreg_acc <- Accuracy(pred4,test$y)
lreg_con <- table(predict=pred4,actual=(test$y))

# Decision Tree
print('Decision Tree')
set.seed(112)

# perform 10-fold cross validation repeated 3 times
caret.control = trainControl(method = 'repeatedcv', number = 10, repeats = 3)
dtree.cv <- train(y ~ ., 
                  data = train,
                  method = "rpart",
                  trControl = caret.control,
                  tuneLength = 15)

dtree.cv
dtree.best <- dtree.cv$finalModel
dtree.best

pred5 <- predict(dtree.cv, test, type = "raw")
dtree_acc <- Accuracy(pred5,test$y)
dtree_con <- table(predict=pred5,actual=(test$y))


# SVM
set.seed(112)
svmfit <- svm(y~.,data=train, kernel ="radial", gamma=1,cost=1)
tune.out <- tune(svm, y~., data=train, kernel ="radial",
              ranges =list(cost=c(0.01, 0.05, .1 ,1 ,10 ,100 ,1000),
                           gamma=c(0.5,1,2,3,4)))
pred6 <- predict(tune.out$best.model,test)

print('We chose radial as the kernel as it best fits the shape of the data 
            and thus should lead to a better prediction.')
svm_acc <- Accuracy(pred6,test$y)
svm_con <- table(predict=pred6,actual=(test$y))

summary(tune.out)


g1 <- ggplot(test, aes(x1,x2,colour=y)) +
  geom_point() +
  ggtitle("True Classes")
g2 <- ggplot(test, aes(x1,x2,colour=pred4)) +
  geom_point() +
  ggtitle("Logistic Regression")
g3 <- ggplot(test, aes(x1,x2,colour=pred5)) +
  geom_point() +
  ggtitle("CV Decision Tree Preds")
g4 <- ggplot(test, aes(x1,x2,colour=pred6)) +
  geom_point() + 
  ggtitle("SVM, CV'd, cost=1, gamma=0.5")

grid.arrange(g1,g2,g3,g4,ncol=2)

sprintf("Logistic Regression Accuracy: %f", lreg_acc) 
sprintf("Decision Tree Accuracy: %f", dtree_acc)
sprintf("SVM Accuracy: %f", svm_acc)

lreg_con
dtree_con
svm_con


```
When Cross Validation was added, SVM still has the highest accuracy, and
Logistic regression still has the lowest at only 50.6%. The accuracy for all is very similar to the previous result.

For bias, SVM was the least biased out of the three with 7 False Positive (FP) and 6 False Negatives (FN). Decision Tree has 33 FP and 14 FN and Logistics Regression has 202 FP and 45 FN, indicating that it was overall more likely to predict 1 instead of 0.

The accuracy of SVM improved slightly with CV, while Decision Tree performed slightly worse.

\pagebreak
### Questions 1.2-1.4 for Varied Data
```{r}
## Question 1.2
set.seed(112)

train_inds <- sample(1:nrow(varied), floor(nrow(varied)*0.5))
train <- varied[ train_inds, ]
test  <- varied[-train_inds, ]

y.train <- train$y
x.train <- model.matrix(y ~ .,train)[,-1]
x.test  <- model.matrix(y ~ .,test)[,-1]

## Question 1.3

# Decision Tree
dtree <- rpart(y~., data=train)
pred2 <- predict(dtree, as.data.frame(x.test), type = "class")
dtree_acc <- Accuracy(pred2,test$y)
dtree_con <- table(predict=pred2,actual=(test$y))

# SVM
svmfit <- svm(y~.,data=train, kernel ="radial", gamma=1,cost=1)
pred3 <- predict(svmfit,as.data.frame(x.test), type="class")
print('We chose radial as the kernel as it best fits the shape of the data 
            and thus should lead to a better prediction.')
svm_acc <- Accuracy(pred3,test$y)
svm_con <- table(predict=pred3,actual=(test$y))

g1 <- ggplot(test, aes(x1,x2,colour=y)) +
  geom_point() +
  ggtitle("True Classes")
g3 <- ggplot(test, aes(x1,x2,colour=pred2)) +
  geom_point() +
  ggtitle("Decision Tree Preds")
g4 <- ggplot(test, aes(x1,x2,colour=pred3)) +
  geom_point() +
  ggtitle("SVM Preds")

grid.arrange(g1,g3,g4,ncol=2)
print('Looking at the three plots, we can see that Logisitc Regression has 
      a lot misclassifications and that Decision Tree performs well, but has noticeable 
      misflassications. Again SVM seems to perform the best.')

sprintf("Decision Tree Accuracy: %f", dtree_acc)
sprintf("SVM Accuracy: %f", svm_acc)
print("In terms of accuracy, Decision Tree performed a little bit better than SVM.")

dtree_con
svm_con
print("Decision Tree was the least biased has it had two false negatives for Class 2.
      SVM had 3 false positives for Class 1.")
```

```{r}
## Question 1.4 for Varied Data

# Decision Tree
print('Decision Tree')
set.seed(112)

# perform 10-fold cross validation repeated 3 times
caret.control = trainControl(method = 'repeatedcv', number = 10, repeats = 3)
dtree.cv <- train(y ~ ., 
                  data = train,
                  method = "rpart",
                  trControl = caret.control,
                  tuneLength = 15)

dtree.cv
dtree.best <- rpart.cv$finalModel
dtree.best

pred5 <- predict(rpart.cv, test, type = "raw")
dtree_acc <- Accuracy(pred5,test$y)
dtree_con <- table(predict=pred5,actual=(test$y))


# SVM
set.seed(112)
svmfit <- svm(y~.,data=train, kernel ="radial", gamma=1,cost=1)
tune.out <- tune(svm, y~., data=train, kernel ="radial",
              ranges =list(cost=c(0.01, 0.05, .1 ,1 ,10 ,100 ,1000),
                           gamma=c(0.5,1,2,3,4)))
pred6 <- predict(tune.out$best.model,test)

print('We chose radial as the kernel as it best fits the shape of the data 
            and thus should lead to a better prediction.')
svm_acc <- Accuracy(pred6,test$y)
svm_con <- table(predict=pred6,actual=(test$y))

summary(tune.out)


g1 <- ggplot(test, aes(x1,x2,colour=y)) +
  geom_point() +
  ggtitle("True Classes")
g3 <- ggplot(test, aes(x1,x2,colour=pred5)) +
  geom_point() +
  ggtitle("CV Decision Tree Preds")
g4 <- ggplot(test, aes(x1,x2,colour=pred6)) +
  geom_point() + 
  ggtitle("SVM, CV'd, cost=1, gamma=0.5")

grid.arrange(g1,g3,g4,ncol=2)
 
sprintf("Decision Tree Accuracy: %f", dtree_acc)
sprintf("SVM Accuracy: %f", svm_acc)

dtree_con
svm_con
```
When Cross Validation was added, both models have high accuracy, with decision tree slightly higher than SVM. Bias was very low for both models.

The accuracy is identical to before CV was added.

\pagebreak
## Question 2: Tree-based methods

### 2.1. Preprocess
```{r}
# 1
library("ISLR")
completeRows <- complete.cases(Hitters)
hitters <- Hitters[completeRows,]
hitters$Salary <- log(hitters$Salary) # Q2 (Converted to log before dataset is split)

Heart <- read.csv("data/Heart.csv")[-1] # Q3 (removed row identifier)
completeHeartRows <- complete.cases(Heart)
heart <- Heart[completeHeartRows, ]
heart$AHD <- as.factor(heart$AHD)

set.seed(112)
train_inds <- sample(1:nrow(hitters), floor(nrow(hitters)*0.7))
train.hitters <- hitters[ train_inds, ]
test.hitters  <- hitters[-train_inds, ]

train_inds <- sample(1:nrow(heart), floor(nrow(heart)*0.7))
train.heart <- heart[ train_inds, ]
test.heart  <- heart[-train_inds, ]

hitters
heart
```

### 2.2. Decision Trees for Regression
```{r}
# 1
set.seed(112)

dtree_hitters <- rpart(Salary ~ Hits + Years, data=train.hitters)

# 2
fancyRpartPlot(dtree_hitters, caption ="")

# 3
print("Based on the decision tree, the output is the node labelled 7.
      The player's salary should be around 6.7")

# 4
preds2.2 <- predict(dtree_hitters,test.hitters, type="vector")
RMSE(preds2.2,test.hitters$Salary)
preds2.2[0:4]

sum((test.hitters$Salary - preds2.2)^2)

```

### 2.3. Decision Trees for Classification
```{r}
# 1
set.seed(112)

dtree_heart <- rpart(AHD ~ ., data=train.heart)

# 2
fancyRpartPlot(dtree_heart, caption ="")

# 3
preds2.3 <- predict(dtree_heart,test.heart, type="class")
accuracy <- Accuracy(preds2.3,test.heart$AHD)
accuracy

# 4
conf <- ConfusionMatrix(preds2.3, test.heart$AHD)
conf

```

### 2.4. Bagging: Regression 
```{r}
library(randomForest)
library(apricom)
set.seed(112)

# 1
print("Filtering out the NA values is done in the pre-processing step")

# 2
hitters.bag <- randomForest(Salary ~ . , data = train.hitters,mtry = ncol(train.hitters)-1)
plot(hitters.bag)

# 3
preds.hittersBag <- predict(hitters.bag,test.hitters)
preds.hittersBag[0:4]

# 4
sse.bagging <- sum((test.hitters$Salary - preds.hittersBag)^2)
sse.bagging

# 5
print("The SSE from bagging is 25.48093 and is lower than the SSE from regression tree which is 32.9608 ")
```

### Question 2.5. Bagging: Classification
```{r}
set.seed(112)

# 1
print("Filtering out the NA values is done in the pre-processing step")

# 2
heart.bag <- randomForest(AHD ~ . , data = train.heart)
plot(heart.bag)

# 3
preds.heartBag <- predict(heart.bag, test.heart, type = "class")
preds.heartBag[0:4]

# 4
Accuracy(preds.heartBag,test.heart$AHD) 
ConfusionMatrix(preds.heartBag, test.heart$AHD)

# 5
print("The accuracy from bagging is 88.89% which is higher than the accuracy 
      from classification tree which is 87.78%")
```

### Question 2.6. Random Forest: Regression
```{r}
set.seed(21)

# 1
hitters.26 <- na.roughfix(Hitters)
train_inds26 <- sample(1:nrow(hitters.26), floor(nrow(hitters.26)*0.7))
train.hitters26 <- hitters.26[ train_inds26, ]
test.hitters26  <- hitters.26[-train_inds26, ]

# 2
m <- ceiling(sqrt(ncol(train.hitters)))
hitters.forest <- randomForest(Salary ~ . , data = train.hitters, mtry = m, importance=T)
plot(hitters.forest)

# 3
preds.hittersForest <- predict(hitters.forest,test.hitters)
preds.hittersForest[0:4]

# 4
sse.forest <- sum((test.hitters$Salary - preds.hittersForest)^2)
sse.forest

# 5
print("The SSE from the random forest is 22.74008 which is lower than both the SSE from bagging (which is 25.48093) and the SSE from regression tree (which is 32.9608)")

```