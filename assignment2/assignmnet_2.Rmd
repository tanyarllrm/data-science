---
title: 'Lab Report 2'
author: "By: Abhishek Shah, Ravi Seth, Tanya Ralliaram"
geometry: margin=.75in
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
    theme: cosmo
header-includes:
- \usepackage{graphicx}
- \usepackage{color}
graphics: yes
fontsize: 11pt
always_allow_html: true
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r, message=FALSE}
# Insert necessary packages
library('glmnet')
library('caret')
library('ISLR')
```

## Question 1: Nonlinear Regression


\pagebreak
## Question 2: Text Classification
```{r}
# read in data
health <- read.csv("mental_health.csv")[,-1]
```


### 2.1 Train / Test Split
```{r}
set.seed(123)

train_inds <- sample(1:nrow(health), floor(nrow(health)*0.8))
train <- health[ train_inds, ]
test  <- health[-train_inds, ]

X_train   <- model.matrix(IsMentalHealthRelated ~ .,train)
y_train   <- train$IsMentalHealthRelated
X_test   <- model.matrix(IsMentalHealthRelated ~ .,test)
y_test   <- test$IsMentalHealthRelated

cat('train: ', dim(train), ', test: ', dim(test))
```

### 2.2 Fit models
```{r, warning=FALSE}
# Logistic Regression model
fit.logreg <- glm(formula = IsMentalHealthRelated ~ ., data=train, family = binomial())

# L1 Model
cv.fit  <- cv.glmnet(X_train, y_train, alpha=1, family="binomial", nfolds = 5)
lambda.l1 <- cv.fit$lambda.min
fit.l1 <- glmnet(X_train, y_train, alpha=1, family="binomial", lambda=lambda.l1)

# L2 Model
cv.fit  <- cv.glmnet(X_train, y_train, alpha=0, family="binomial", nfolds = 5)
lambda.l2 = cv.fit$lambda.min
fit.l2 <- glmnet(X_train, y_train, alpha=0, family="binomial", lambda=lambda.l2)
```

### 2.3 Compare Performances
```{r, warning=FALSE}
# Logistic Regression (LR)
probs.logreg <- predict(fit.logreg, as.data.frame(X_test), type="response")
preds.logreg  <- ifelse(probs.logreg >= 0.5, 1, 0)
acc.logreg <- mean(preds.logreg == y_test)

# L1 Model
probs.l1 <- predict(fit.l1, X_test, type="response")
preds.l1  <- ifelse(probs.l1 >= 0.5, 1, 0)
acc.l1 <- mean(preds.l1 == y_test)

# L2 Model
probs.l2 <- predict(fit.l2, X_test, type="response")
preds.l2  <- ifelse(probs.l2 >= 0.5, 1, 0)
acc.l2 <- mean(preds.l2 == y_test)

cat(sprintf("Logisitc Regression Accuracy: %f \nL1 Accuracy: %f \nL2 Accuracy: %f", acc.logreg, acc.l1, acc.l2))
```
The L2 model had the best accuracy and L1 has the second best accuracy. Logistic Regression without any regularization had the worst accuracy out of the three.

### 2.4 Interpret the models
```{r}
sorted.l1 <- sort(coef(fit.l1)[,1])
cat('The words that have the highest coefficients with L1 are: \n')
sort(tail(sorted.l1, 5), decreasing=TRUE)
cat('\nThe words that have the smallest coefficients with L1 are: \n')
head(sorted.l1, 5)
sorted.l2 <- sort(coef(fit.l2)[,1])
cat('\nThe words that have the highest coefficients with L2 are: \n')
sort(tail(sorted.l2, 5), decreasing=TRUE)
cat('\nThe words that have the smallest coefficients with L2 are: \n')
head(sorted.l2, 5)

```

L1 tends to tends to zero many coefficients while keeping the rest as they are.
L2 tends to shrink all the coefficients and doesnâ€™t zero any.

\pagebreak
## Question 3: Subset Selection

