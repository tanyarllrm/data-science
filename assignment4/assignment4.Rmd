---
title: 'Lab Report 3'
author: "By: Abhishek Shah, Ravi Seth, Tanya Ralliaram"
geometry: margin=.75in
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
    theme: cosmo
header-includes:
- \usepackage{graphicx}
- \usepackage{color}
graphics: yes
fontsize: 11pt
always_allow_html: true
---

```{r, message=FALSE}
# Insert necessary packages
library(keras)
library(tidyverse)
library(neuralnet)
```

## Question 1: Classification using NNets
```{r}
mnist <- dataset_fashion_mnist()
```

### 1.1: Get Data
```{r}
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

dim(x_train)
dim(x_test)
```

### 1.2: Plot
```{r}
par(pty="s") # for keeping the aspect ratio 1:1
trouser <- x_train[107,28:1,1:28]
image(t(trouser), col = gray.colors(256), axes = FALSE)
bag <- x_train[213,28:1,1:28]
image(t(bag), col = gray.colors(256), axes = FALSE)
boot <- x_train[1,28:1,1:28]
image(t(boot), col = gray.colors(256), axes = FALSE)
```

### 1.3: Process the dataset
```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# rescale
x_train <- x_train / 255
x_test <- x_test / 255

y_train <- to_categorical(y_train, 10)
y_test  <- to_categorical(y_test, 10)
```

### 1.4: Fit a Shallow Network
```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dense(units = 10, activation = 'softmax')
#summary(model)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

history <- model %>% fit(
  x_train, y_train, 
  epochs = 10, 
  batch_size = 128, 
  validation_split = 0.2
)

model %>% evaluate(x_test, y_test)

```
The settings that performed the best were using 256 neurons for the hidden layer and using 'relu' as the activation function.

### 1.5: Fit a Deep Neural Network
```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dense(units = 256, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')
  
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

history <- model %>% fit(
  x_train, y_train, 
  epochs = 10, 
  batch_size = 128, 
  validation_split = 0.2
)
model %>% evaluate(x_test, y_test)
```

The model that seems to generate the best test accuracy is using 256 neurons for both hidden layers and using 'relu' as the activation function.